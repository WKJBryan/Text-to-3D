# ğŸš€ Enhanced RAG + Intelligent AI Manufacturing Pipeline

An advanced AI-powered 3D CAD generation system that combines Retrieval-Augmented Generation (RAG) with intelligent reasoning to create CadQuery models through natural conversation.

## âœ¨ Key Features

### ğŸ§  **Object-First Intelligent Conversation Flow**
- **Immediate object detection** from user's first message
- **Smart parameter extraction** from reference examples
- **Context-aware questioning** based on actual object requirements
- **Adaptive code generation** with feature removal/addition

### ğŸ”„ **Dual-Mode Generation System**
- **RAG Mode**: Uses hierarchical reference examples when relevant
- **Intelligent Reasoning**: Falls back to engineering principles for novel designs
- **Toggle Control**: Switch between modes to compare approaches

### ğŸ“š **Hierarchical Reference Library**
- 16+ pre-built references across 4 complexity levels
- Semantic search with similarity scoring
- Automatic complexity-based selection
- Categories: Primitive, Functional, Mathematical, Manufacturing

## ğŸ¯ System Innovation

### Traditional CAD Chatbot Flow âŒ
```
User: "I want a cup"
Bot: "What dimensions?" (generic)
User: "100mm height, 60mm diameter"
Bot: "What thickness?" (still generic)
â†’ Finally searches for examples
â†’ Often fails to match
```

### Our Object-First Flow âœ…
```
User: "I want a cup"
â†’ Immediate RAG search for "cup"
â†’ Finds "mug" reference (0.534 similarity)
â†’ Extracts actual parameters: mug_radius, mug_height, handle
Bot: "For your cup, what radius (default 40mm) and height (default 90mm)? Want a handle?"
â†’ Generates using adapted reference
```

## ğŸ“‹ Prerequisites

- Python 3.11
- [Ollama](https://ollama.ai/) installed and running
- CUDA-capable GPU (optional, for faster inference)
- 8GB+ RAM recommended

## ğŸ› ï¸ Installation

1. **Clone the repository**
```bash
git clone https://github.com/WKJBryan/Text-to-3D.git
cd Text-to-3D
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Install and start Ollama**
```bash
# Install Ollama (Mac/Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama server
ollama serve

# Pull the default model
ollama pull llama3.1:8b
```

5. **Configure environment**
```bash
cp .env.example .env
# Edit .env with your preferences
```

## ğŸš€ Quick Start

### Desktop Application
```bash
python main.py
```

### Test Scripts
```bash
# Test RAG search functionality
python test_rag_search.py

# Test handle removal feature
python test_handle_removal.py

# Test ring generation
python test_ring_generation.py
```

## ğŸ’¡ Usage Examples

### Simple Object Generation
```
User: "I need a gear"
Assistant: "For your spur gear, how many teeth? (default: 20)"
User: "24 teeth"
Assistant: "What module? (default: 2.5mm)"
User: "3mm"
â†’ Generates involute gear with exact specifications
```

### Feature Removal
```
User: "I want a cup"
Assistant: "What height and radius for your cup? Do you want a handle?"
User: "100mm height, 30mm radius, no handle"
â†’ Generates cup without handle code
```

### Complex Objects
```
User: "I need a phone stand"
â†’ Finds phone_stand reference
â†’ Asks about angle, phone size, charging port
â†’ Generates with stability features and cable management
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Desktop UI (CustomTkinter)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Intelligent Conversation Assistant       â”‚
â”‚  â€¢ Object-first detection                    â”‚
â”‚  â€¢ Smart parameter extraction                â”‚
â”‚  â€¢ Context-aware questioning                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAG Generator â”‚ â”‚   LLM Engine    â”‚
â”‚ â€¢ Semantic     â”‚ â”‚ â€¢ Ollama API    â”‚
â”‚   search       â”‚ â”‚ â€¢ Generation    â”‚
â”‚ â€¢ Code adapt   â”‚ â”‚ â€¢ Temperature   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Reference Library               â”‚
â”‚ â€¢ Simple: box, cylinder, sphere    â”‚
â”‚ â€¢ Medium: phone_stand, mug, bracketâ”‚
â”‚ â€¢ Complex: gear, spring, thread    â”‚
â”‚ â€¢ Advanced: living_hinge, snap_fit â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš™ï¸ Configuration

### Environment Variables (.env)
```env
# Model Configuration
DEFAULT_MODEL=llama3.1:8b
PERFORMANCE_MODE=balanced  # fast, balanced, quality

# RAG Configuration
EMBEDDING_MODEL=all-MiniLM-L6-v2
SIMILARITY_THRESHOLD=0.3
VECTOR_SEARCH_TOP_K=3

# UI Configuration
WINDOW_SIZE=900x700
THEME=dark

# Export Configuration
EXPORT_DIRECTORY=./exports
```

### Performance Modes
- **fast**: Quick responses, smaller context (512 tokens)
- **balanced**: Default, good for most uses (1024 tokens)
- **quality**: Best results, larger context (2048 tokens)

## ğŸ“ Project Structure

```
enhanced-rag-manufacturing/
â”œâ”€â”€ main.py                 # Entry point
â”œâ”€â”€ desktop_app.py          # UI application
â”œâ”€â”€ .env                    # Configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ assistant.py        # Conversation logic
â”‚   â”œâ”€â”€ llm_engine.py       # Ollama interface
â”‚   â””â”€â”€ generation/
â”‚       â”œâ”€â”€ RAG_generator.py      # Code generation
â”‚       â”œâ”€â”€ reference_library.py  # Example library
â”‚       â””â”€â”€ export.py             # STL/STEP export
â”œâ”€â”€ exports/                # Generated models
â”œâ”€â”€ embeddings/             # Cached embeddings
â””â”€â”€ tests/                  # Test scripts
```

## ğŸ” How RAG Works

### 1. **Semantic Search**
- User input â†’ Embedding model â†’ Vector representation
- Searches reference library using FAISS
- Returns top-k similar examples with scores

### 2. **Similarity-Based Strategy**
```
â‰¥ 0.5  â†’ Direct adaptation (very similar)
0.3-0.5 â†’ Pattern combination (good match)
0.2-0.3 â†’ Category adaptation (related)
< 0.2  â†’ Intelligent reasoning (no match)
```

### 3. **Code Adaptation**
- LLM adapts reference code to user specifications
- Handles feature removal (no handle, plain, simple)
- Updates dimensions and parameters
- Maintains manufacturing constraints

## ğŸ› Troubleshooting

### Ollama Connection Error
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If not, start it
ollama serve
```

### Model Not Found
```bash
# List available models
ollama list

# Pull required model
ollama pull llama3.1:8b
```

### Slow Generation
- Switch to "fast" mode in .env
- Use smaller model (llama2:7b)
- Reduce VECTOR_SEARCH_TOP_K
- Ensure GPU acceleration is working

### RAG Search Issues
```bash
# Rebuild embeddings
python -c "from src.generation.RAG_generator import EnhancedRAGCADGenerator; g = EnhancedRAGCADGenerator(None); g.rebuild_embeddings()"
```

## ğŸš§ Advanced Features

### Adding Custom References
```python
from src.generation.RAG_generator import EnhancedRAGCADGenerator

generator = EnhancedRAGCADGenerator(llm_engine)
generator.add_reference_example(
    name="custom_part",
    description="My custom mechanical part",
    code=cadquery_code,
    complexity="medium",
    category="functional"
)
```

### Custom LLM Models
```bash
# Use different Ollama model
ollama pull mistral:latest

# Update .env
DEFAULT_MODEL=mistral:latest
```

## ğŸ“Š Performance Metrics

- **Object Detection**: ~100ms
- **RAG Search**: ~200ms
- **LLM Generation**: 2-10s (depends on model/mode)
- **Code Execution**: ~500ms
- **Total Response**: 3-15s typical

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- Additional reference examples
- Support for more CAD formats
- Multi-language support
- Cloud deployment options
- Web interface

## ğŸ“„ License

GNU AFFERO GENERAL PUBLIC LICENSE Version 3 - See LICENSE file for details

## ğŸ™ Acknowledgments

- [CadQuery](https://github.com/CadQuery/cadquery) - Parametric CAD scripting
- [Ollama](https://ollama.ai/) - Local LLM deployment
- [Sentence-Transformers](https://www.sbert.net/) - Semantic embeddings
- [FAISS](https://github.com/facebookresearch/faiss) - Vector similarity search

## ğŸ“ Support

- **Issues**: GitHub Issues
- **Discussions**: GitHub Discussions
- **Email**: bryan_wang@mymail.sutd.edu.sg

## ğŸ”® Future Roadmap

- [ ] Assembly support (multi-part objects)
- [ ] Constraint solver integration
- [ ] Real-time collaborative design
- [ ] Voice input support
- [ ] AR/VR visualization
- [ ] Cloud-based rendering
- [ ] Fine-tuned CAD-specific LLM

---

**Built with â¤ï¸ for the maker community**
